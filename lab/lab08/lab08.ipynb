{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab08.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 8: Loss Minimization\n",
    "\n",
    "## Modeling, Estimation, and Gradient Descent\n",
    "\n",
    "## This Assignment\n",
    "In this notebook, we explore modeling data, estimating optimal parameters, and a numerical estimation method called gradient descent. These concepts are some of the fundamentals of data science and machine learning and will serve as the building blocks for future projects, classes, and work.\n",
    "\n",
    "In this discussion, you will:\n",
    "\n",
    "- Practice reasoning about a model\n",
    "\n",
    "- Build some intuition for loss functions and how they behave \n",
    "\n",
    "- Work through deriving the gradient of a loss with respect to model parameters\n",
    "\n",
    "- Work through a basic version of gradient descent.\n",
    "\n",
    "This discussion is comprised of completing code, deriving analytic solutions, writing LaTeX and visualizing loss.\n",
    "\n",
    "This assignment should be completed and submitted before Tuesday, July 23, 2019 at 11:59 PM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8cbb07a2cc27f7d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "# Set some parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 16\n",
    "np.set_printoptions(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "utils-code",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use plot_3d helper function to help us visualize gradient\n",
    "from lab08_utils import plot_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Load Data\n",
    "Load the data.csv file into a pandas dataframe.  \n",
    "Note that we are reading the data directly from the URL address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load our sample data\n",
    "data = pd.read_csv(\"https://github.com/DS-100/su19/raw/gh-pages/assets/datasets/disc08_data.csv\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## A Simple Model\n",
    "Let's start by examining our data and creating a simple model that can represent this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "First, let's visualize the data in a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def scatter(x, y):\n",
    "    \"\"\"\n",
    "    Generate a scatter plot using x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ...\n",
    "    plt.scatter(x, y, marker='.')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "The data looks roughly linear, with some extra sinusoidal noise. For now, let's assume that the data follows some underlying linear model. We define the underlying linear model that predicts the value $y$ using the value $x$ as: $f_{\\theta^*}(x) = \\theta^* \\cdot x$\n",
    "\n",
    "Since we cannot find the value of the population parameter $\\theta^*$ exactly, we will assume that our dataset approximates our population and use our dataset to estimate $\\theta^*$. We denote an estimate with $\\theta$ and the fitted estimated chosen based on the data as $\\hat{\\theta}$. Our parameterized model is:\n",
    "\n",
    "$$\\Large\n",
    "f_{\\theta}(x) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "Based on this equation, we will define the linear model function `linear_model` below to estimate $\\textbf{y}$ (the $y$-values) given $\\textbf{x}$ (the $x$-values) and $\\theta$. This model is similar to the model you defined in the Modeling and Estimation lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, theta):\n",
    "    \"\"\"\n",
    "    Returns the estimate of y given x and theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- the scalar theta\n",
    "    \"\"\"\n",
    "    return theta * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In class, we learned that the squared loss function is smooth and continuous. Let's use squared loss to evaluate our estimate $\\theta$, which we will use later to identify an optimal $\\theta$, denoted $\\hat{\\theta}$. For several points, we compute the average loss or *empirical risk*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def average_squared_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Returns the averge squared loss for observations y and predictions y_hat.\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- the vector of true values y\n",
    "    y_hat -- the vector of predicted values y_hat\n",
    "    \"\"\"\n",
    "    return np.mean(np.square(y - y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Finally, we will visualize the average squared loss as a function of $\\theta$, where several different values of $\\theta$ are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize(x, y, thetas):\n",
    "    \"\"\"\n",
    "    Plots the average l2 loss for given x, y as a function of theta.\n",
    "    Use the functions you wrote for linear_model and l2_loss.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    thetas -- an array containing different estimates of the scalar theta\n",
    "    \"\"\" \n",
    "    avg_loss = np.array([average_squared_loss(linear_model(x, theta), y) for theta in thetas])\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(thetas, avg_loss)\n",
    "    plt.xlabel(\"Theta\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    \n",
    "thetas = np.linspace(-1, 5, 70)\n",
    "visualize(x, y, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that $\\hat{\\theta}$ is approximately 1.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1: Fitting our Simple Model\n",
    "Now that we have defined a simple linear model and loss function, let's begin working on fitting our model to the data.\n",
    "\n",
    "### Question 1\n",
    "Let's confirm our visual findings for optimal $\\hat{\\theta}$.\n",
    "\n",
    "#### Question 1a\n",
    "First, find the analytical solution for the optimal $\\hat{\\theta}$ for average squared loss. Write up your solution in the cell below using LaTeX.\n",
    "\n",
    "Hint: notice that we now have $\\textbf{x}$ and $\\textbf{y}$ instead of $x$ and $y$. This means that when writing the empirical risk function $R(\\textbf{x}, \\textbf{y}, \\theta)$, you'll need to take the average of the squared losses for each $y_i$, $f_\\theta(x_i)$ pair. For tips on getting started, see chapter 10 of the textbook (https://www.textbook.ds100.org/ch/10/modeling_loss_functions.html). Note that if you click \"Open in DataHub\", you can access the LaTeX source code of the book chapter, which you might find handy for typing up your work. Show your work, i.e. don't just write the answer.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1a\n",
    "manual: True\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2a-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 1b\n",
    "Now that we have the analytic solution for $\\hat{\\theta}$, implement the function `find_theta` that calculates the numerical value of $\\hat{\\theta}$ based on our data $\\textbf{x}$, $\\textbf{y}$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_theta(x, y):\n",
    "    \"\"\"\n",
    "    Find optimal theta given x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    theta_hat = ...\n",
    "    return theta_hat\n",
    "theta_hat = find_theta(x, y)\n",
    "print(f'theta_hat = {theta_hat}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q1b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now, let's plot our risk function again using the `visualize` function. But this time, we will add a vertical line at the optimal value of theta (plot the line $x = \\hat{\\theta}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = find_theta(x, y)\n",
    "visualize(x, y, thetas)\n",
    "plt.axvline(x=theta_opt, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We now have an optimal value for $\\theta$ that minimizes the empirical risk. We can use the scatter plot of the data and add the line $f_{\\hat{\\theta}}(x) = \\hat{\\theta} \\cdot \\textbf{x}$ using the $\\hat{\\theta}$ computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = find_theta(x, y)\n",
    "scatter(x, y)\n",
    "line_values = linear_model(x, theta_opt)\n",
    "plt.plot(x, line_values, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Great! It looks like our estimator $f_{\\hat{\\theta}}(x)$ is able to estimate the average y for each x quite well using a single parameter $\\theta$. Now let's try to remove the linear portion of our model from the data to see if we missed anything. \n",
    "\n",
    "The remaining data is known as the residual, $\\textbf{r}=\\textbf{y}-\\hat{\\theta} \\cdot \\textbf{x}$. Below, we find the residual and plot the residuals corresponding to $x$ in a scatter plot. We also plot a horizontal line at $y=0$ to assist visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_residual(x, y):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of the residuals, the remaining \n",
    "    values after removing the linear model from our data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    ...\n",
    "    theta_hat = find_theta(x, y)\n",
    "    y_sin = y - linear_model(x, theta_hat)\n",
    "    plt.scatter(x, y_sin)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('residual (true y - estimated y)')\n",
    "    plt.title('Residual vs x for Linear Model')\n",
    "    plt.axhline(y=0, color='r')\n",
    "\n",
    "visualize_residual(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 2: Increasing Model Complexity\n",
    "\n",
    "It looks like the remaining data is sinusoidal, meaning our original data follows a linear function and a sinusoidal function. Let's define a new model to address this discovery and find optimal parameters to best fit the data:\n",
    "\n",
    "$$\\Large\n",
    "f_\\boldsymbol\\theta(x) = \\theta_1x + sin(\\theta_2x)\n",
    "$$\n",
    "\n",
    "Now, our model is parameterized by both $\\theta_1$ and $\\theta_2$, or composed together, $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Note that a generalized sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. Looking at the residual plot above, it looks like the residual is zero at x = 0, and the residual swings between -1 and 1. Thus, it seems reasonable to effectively set the scaling and phase shifting parameter ($a$ and $c$ in this case) to 1 and 0 respectively. While we could try to fit $a$ and $c$, we're unlikely to get much benefit. When you're done with the discussion, you can try adding $a$ and $c$ to our model and fitting these values to see if you can get a better loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We define the `sin_model` function below that predicts $\\textbf{y}$ (the $y$-values) using $\\textbf{x}$ (the $x$-values) based on our new equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_model(x, theta_1, theta_2):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta_1 -- the scalar value theta_1\n",
    "    theta_2 -- the scalar value theta_2\n",
    "    \"\"\"\n",
    "    return theta_1 * x + np.sin(theta_2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 2a\n",
    "Use the average squared loss to compute $\\frac{\\partial R }{\\partial \\theta_1}, \\frac{\\partial R }{\\partial \\theta_2}$. \n",
    "\n",
    "First, we will use LaTex to write $R(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2)$, $\\frac{\\partial R }{\\partial \\theta_1}$, and $\\frac{\\partial R }{\\partial \\theta_2}$ given $\\textbf{x}$, $\\textbf{y}$, $\\boldsymbol{\\theta}$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: True\n",
    "-->\n",
    "<!-- EXPORT TO PDF -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q3b-answer",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "*Write your answer here, replacing this text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 2b\n",
    "Now, implement the functions `dt1` and `dt2`, which should compute $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ respectively. Use the formulas you wrote for $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ in the previous exercise. In the functions below, the parameter `theta` is a vector that looks like $( \\theta_1, \\theta_2 )$.\n",
    "\n",
    "Note: To keep your code a bit more concise, be aware that `np.mean` does the same thing as `np.sum` divided by the length of the numpy array.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3c-answer-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dt1(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "def dt2(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def dt(x, y, theta):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    return np.array([dt1(x,y,theta), dt2(x,y,theta)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q2b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3: Gradient Descent\n",
    "Now try to solve for the optimal $\\hat{\\theta}$ analytically...\n",
    "\n",
    "**Just kidding!**\n",
    "\n",
    "You can try but we don't recommend it. When finding an analytic solution becomes difficult or impossible, we resort to numerical optimization. \n",
    "\n",
    "### Question 3\n",
    "\n",
    "So let's try implementing a numerical optimization method: gradient descent!\n",
    "\n",
    "#### Question 3a\n",
    "Implement the `grad_desc` function that performs gradient descent for a finite number of iterations. This function takes in an array for $\\textbf{x}$ (`x`), an array for $\\textbf{y}$ (`y`), and an initial value for $\\theta$ (`theta`). `alpha` will be the learning rate (or step size, whichever term you prefer). In this part, we'll use a static learning rate that is the same at every time step.\n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Also at each time step, be sure to save the current `theta` in `theta_history`, along with the average squared loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "Hints:\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times\n",
    "- Recall that the gradient descent update function follows the form:\n",
    "$$\\large\n",
    "\\boldsymbol\\theta^{(t+1)} \\leftarrow \\boldsymbol\\theta^{(t)} - \\alpha \\left(\\nabla_\\boldsymbol\\theta \\mathbf{R}(\\textbf{x}, \\textbf{y}, \\boldsymbol\\theta^{(t)}) \\right)\n",
    "$$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "def init_t():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\"\n",
    "    return np.zeros((2,))\n",
    "\n",
    "def grad_desc(x, y, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t, num_iter=20, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 3b\n",
    "Now, let's try using a decaying learning rate. Implement `grad_desc_decay` below, which performs gradient descent with a learning rate that decreases slightly with each time step. You should be able to copy most of your work from the previous part, but you'll need to tweak how you update `theta` at each time step.\n",
    "\n",
    "By decaying learning rate, we mean instead of just a number $\\alpha$, the learning should be now $\\frac{\\alpha}{i+1}$ where $i$ is the current number of iteration. (Why do we need to add '+ 1' in the denominator?)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3b\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad_desc_decay(x, y, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and decaying learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "t = init_t()\n",
    "t_est_decay, ts_decay, loss_decay = grad_desc_decay(x, y, t, num_iter=20, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "ok.grade(\"q3b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's visually inspect our results of running gradient descent to optimize $\\boldsymbol\\theta$. Plot our $x$-values with our model's predicted $y$-values over the original scatter plot. You should notice that gradient descent successfully optimized $\\boldsymbol\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me\n",
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t)\n",
    "\n",
    "t = init_t()\n",
    "t_est_decay, ts_decay, loss_decay = grad_desc_decay(x, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c-answer-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = sin_model(x, t_est[0], t_est[1])\n",
    "\n",
    "plt.plot(x, y_pred, label='Model')\n",
    "plt.scatter(x, y, alpha=0.5, label='Observation', color='gold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to see a plot of the loss values over each iteration of gradient descent for both static learning rate and decaying learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(loss)), loss, label='Static Alpha')\n",
    "plt.plot(np.arange(len(loss)), loss_decay, label='Decaying Alpha')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Avg Loss')\n",
    "plt.title('Avg Loss vs Iteration # vs Learning Rate Type')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "You should notice that noth methods minimize loss quickly, although the implementation with a decaying step size appears to converge more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## Visualizing Loss\n",
    "Let's visualize our loss functions and gain some insight as to how gradient descent and stochastic gradient descent are optimizing our model parameters.\n",
    "\n",
    "In the previous plot is about the loss decrease over time, but what exactly is path the theta value? Run the following three cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "ts = np.array(ts).squeeze()\n",
    "ts_decay = np.array(ts_decay).squeeze()\n",
    "loss = np.array(loss)\n",
    "loss_decay = np.array(loss_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see a 3D plot (gradient descent with static alpha)\n",
    "plot_3d(ts[:, 0], ts[:, 1], loss, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d-4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see another 3D plot (gradient descent with decaying alpha)\n",
    "plot_3d(ts_decay[:, 0], ts_decay[:, 1], loss_decay, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Another common way of visualizing 3D dynamics is with a _contour_ plot. Run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Run me\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def contour_plot(title, theta_history, loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    The function takes the following as argument:\n",
    "        theta_history: a (N, 2) array of theta history\n",
    "        loss: a list or array of loss value\n",
    "        loss_function: for example, l2_loss\n",
    "        model: for example, sin_model\n",
    "        x: the original x input\n",
    "        y: the original y output\n",
    "    \"\"\"\n",
    "    theta_1_series = theta_history[:,0] # a list or array of theta_1 value\n",
    "    theta_2_series = theta_history[:,1] # a list or array of theta_2 value\n",
    "\n",
    "    ## In the following block of code, we generate the z value\n",
    "    ## across a 2D grid\n",
    "    t1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1)\n",
    "    t2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1)\n",
    "\n",
    "    x_s, y_s = np.meshgrid(t1_s, t2_s)\n",
    "    data = np.stack([x_s.flatten(), y_s.flatten()]).T\n",
    "    ls = []\n",
    "    for t1, t2 in data:\n",
    "        l = loss_function(model(x, t1, t2), y)\n",
    "        ls.append(l)\n",
    "    z = np.array(ls).reshape(50, 50)\n",
    "    \n",
    "    # Create trace of theta point\n",
    "    # Create the contour \n",
    "    theta_points = go.Scatter(name=\"Theta Values\", \n",
    "                              x=theta_1_series, \n",
    "                              y=theta_2_series,\n",
    "                              mode=\"lines+markers\")\n",
    "    lr_loss_contours = go.Contour(x=t1_s, \n",
    "                                  y=t2_s, \n",
    "                                  z=z, \n",
    "                                  colorscale='Viridis', reversescale=True)\n",
    "\n",
    "    plotly.offline.iplot(go.Figure(data=[lr_loss_contours, theta_points], layout={'title': title}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "contour_plot('Gradient Descent with Static Learning Rate', ts, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Run me\n",
    "contour_plot('Gradient Descent with Decay Learning Rate', ts_decay, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "import jassign.to_pdf\n",
    "jassign.to_pdf.generate_pdf('lab08.ipynb', 'lab08.pdf')\n",
    "ok.submit()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
