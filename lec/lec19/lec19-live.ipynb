{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import warnings\n",
    "# Ignore numpy dtype warnings. These warnings are caused by an interaction\n",
    "# between numpy and Cython and can be safely ignored.\n",
    "# Reference: https://stackoverflow.com/a/40846742\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import nbinteract as nbi\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "np.set_printoptions(threshold=20, precision=2, suppress=True)\n",
    "pd.set_option('display.max_rows', 7)\n",
    "pd.set_option('display.max_columns', 9)\n",
    "pd.set_option('precision', 2)\n",
    "# This option stops scientific notation for pandas\n",
    "# pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "def df_interact(df, nrows=7, ncols=7):\n",
    "    '''\n",
    "    Outputs sliders that show rows and columns of df\n",
    "    '''\n",
    "    def peek(row=0, col=0):\n",
    "        return df.iloc[row:row + nrows, col:col + ncols]\n",
    "\n",
    "    row_arg = (0, len(df), nrows) if len(df) > nrows else fixed(0)\n",
    "    col_arg = ((0, len(df.columns), ncols)\n",
    "               if len(df.columns) > ncols else fixed(0))\n",
    "    \n",
    "    interact(peek, row=row_arg, col=col_arg)\n",
    "    print('({} rows, {} columns) total'.format(df.shape[0], df.shape[1]))\n",
    "\n",
    "def display_df(df, rows=50, cols=pd.options.display.max_columns):\n",
    "    with pd.option_context('display.max_rows', rows,\n",
    "                           'display.max_columns', cols):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nba.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WON'] = df['WL'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't what we're actually interested in. Why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we bin the data, then take the proportion of won games in each bin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = pd.cut(games['FG_PCT_DIFF'], 20)\n",
    "mids = [(b.left + b.right)/2 for b in bins]\n",
    "games['WON'].groupby(mids).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_rates = games['WON'].groupby(mids).agg(lambda s: sum(s)/len(s))\n",
    "win_rates.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to model the relationship between vector-valued $X$ and the conditional distribution of a binary categorical $Y \\in {0, 1}$ is to assume that the log odds ratio is linear in $X$:\n",
    "\n",
    "$$\\log \\left( \\frac{P(Y=1|X)}{P(Y=0|X)} \\right) = X \\cdot \\theta$$\n",
    "\n",
    "Abbreviate $P(Y=1|X)$ as $p$ and $X \\cdot \\theta$ as $t$, then:\n",
    "\n",
    "\\begin{align*}\n",
    "\\log \\left( \\frac{P(Y=1|X)}{P(Y=0|X)} \\right) &= X \\cdot \\theta && \\text{} \\\\[10pt]\n",
    "\\log \\left( \\frac{p}{1-p} \\right) &= t && \\text{; Abbreviate and complement rule} \\\\[10pt]\n",
    "\\frac{p}{1-p} &= \\exp(t) && \\text{; exponentiate both sides} \\\\[10pt]\n",
    "p &= \\exp(t) - p \\exp(t) && \\text{; multiply by $1-p$} \\\\[10pt]\n",
    "p (1 + \\exp(t)) &= \\exp(t) && \\text{; add $p \\exp(t)$ and factor out $p$} \\\\[10pt]\n",
    "p &= \\frac{\\exp(t)}{1 + \\exp(t)} && \\text{; divide by $1 + \\exp(t)$} \\\\[10pt]\n",
    "p &= \\frac{1}{1 + \\exp(-t)} && \\text{; multiply by $\\frac{\\exp(-t)}{\\exp(-t)}$} \\\\[10pt]\n",
    "P(Y=1|X) &= \\frac{1}{1 + \\exp(-X \\cdot \\theta)} && \\text{; Unabbreviate}\n",
    "\\end{align*}\n",
    "\n",
    "This transformation is called the logistic function, traditionally denoted $\\sigma$ (sigma).\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + \\exp(-t)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(t):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(li): \n",
    "    return [item for sub in li for item in sub]\n",
    "\n",
    "bs = [-2, -1, -0.5, 2, 1, 0.5]\n",
    "xs = np.linspace(-10, 10, 100)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(10, 6))\n",
    "for ax, b in zip(flatten(axes), bs):\n",
    "    ys = sigma(xs * b)\n",
    "    ax.plot(xs, ys)\n",
    "    ax.set_title(r'$ \\theta = $' + str(b))\n",
    "\n",
    "# add a big axes, hide frame\n",
    "fig.add_subplot(111, frameon=False)\n",
    "# hide tick and tick label of the big axes\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False,\n",
    "                left=False, right=False)\n",
    "plt.grid(False)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel(r'$ \\frac{1}{1+\\exp(-\\theta \\cdot x)} $')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Loss (a.k.a., Cross-Entropy Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = pd.DataFrame({\n",
    "    'X': [-5, -4, -3, -2, -1, 1, 2, 3, 4, 5],\n",
    "    'Y': [ 0,  0,  0,  0,  1, 0, 1, 1, 1, 1]\n",
    "})\n",
    "plt.scatter(toy['X'], toy['Y']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(toy['X'], toy['Y']);\n",
    "plt.plot(toy['X'], sigma(toy['X']));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_risk(theta, loss):\n",
    "    pass\n",
    "\n",
    "def squared_loss(y, y_hat):\n",
    "    pass\n",
    "\n",
    "bs = np.linspace(-4, 4, 50)\n",
    "\n",
    "plt.plot(bs, [empirical_risk(b, squared_loss) for b in bs]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix an individual and let $\\hat{p}$ be our predicted probability of the correct class for that individual. If the individual's class is 1, then $\\hat{p}$ is our predicted chance that their class is 1, and if the individual's class is 0, then $\\hat{p}$ is our predicted chance that their class is 0.\n",
    "\n",
    "Regardless of whether the individual's class is 1 or 0, $\\hat{p}$ is the predicted chance of the *correct* class. So here are some reasonable stipulations for a loss function.\n",
    "\n",
    "- If $\\hat{p} = 1$ then there should be no loss.\n",
    "- If $\\hat{p}$ is high, the loss should be small.\n",
    "- If $\\hat{p}$ is low, the loss should be large.\n",
    "- Whatever $\\hat{p}$ is, the loss should be non-negative.\n",
    "\n",
    "Because $\\hat{p}$ is between 0 and 1, the function $\\hat{p} \\rightarrow -\\log(\\hat{p})$ has all of these properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat = np.arange(0.001, 0.999, 0.01)\n",
    "loss = -np.log(p_hat)\n",
    "plt.plot(p_hat, loss, color='k')\n",
    "plt.xlabel('$\\hat{p}$: Predicted Chance of Correct Class')\n",
    "plt.ylabel('$-\\log(\\hat{p})$')\n",
    "plt.title('Loss for One Individual');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other functions that have these properties. But this one has other nice properties and interpretations in the context of predictions. You might see some of them in more advanced classes, and you will see one in this class as soon as we start calculating gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted chance of class 1.\n",
    "x = np.arange(-5, 5, 0.01)\n",
    "px = sigma(x)\n",
    "plt.plot(x, px, color = 'darkblue')\n",
    "\n",
    "def add_points(xs, y, color):\n",
    "    plt.scatter(xs, [y] * len(xs), color=color, s=40)\n",
    "    for x in xs:\n",
    "        plt.plot([x, x], [1-y, sigma(x)], color=color)\n",
    "\n",
    "add_points(np.array([-5, -4, -3, -2, 1]), 0, 'gold')\n",
    "add_points(np.array([-1, 2, 3, 4, 5]), 1, 'darkblue')\n",
    "        \n",
    "plt.xlabel('x')\n",
    "plt.title('Predicted Chance of Correct Class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A compact way to write this loss in a single expression, where $y_i$ is the class for individual $i$, is:\n",
    "\n",
    "$$\n",
    "-y_i\\log(p(x_i)) - (1-y_i)\\log(1-p(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, y_hat):\n",
    "    ...\n",
    "\n",
    "plt.plot(bs, [empirical_risk(b, log_loss) for b in bs]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot('FG_PCT_DIFF', 'WON', \n",
    "              kind='reg', y_jitter=0.1,\n",
    "              logistic=True, ci=None,\n",
    "              data=games);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta):\n",
    "    pass\n",
    "\n",
    "def risk(theta):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
