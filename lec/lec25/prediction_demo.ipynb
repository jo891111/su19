{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for \"Prediction: Regression and Classification\" Lecture\n",
    "*Prepared by Philippe Boileau*\n",
    "\n",
    "This notebook contains the code for perfoming the analyses for the case studies introduced during the \"Prediction: Classification and Regression\" lecture. \n",
    "\n",
    "**Note.** Prediction methods such as classification and regression trees typically have many tuning parameters/inputs and output values. There are also differences in implementation across software packages. Make sure to consult the documentation to understand how the predictors are built and how to interpret the results. Try modifying tuning parameters for each of the predictors below to see how this influences the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tabulate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.api.types import CategoricalDtype\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Rent Using Craigslist Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using data recently collected from Craigslist, we will attempt to predict the rent price of apartments in Berkeley based on the following five features: Square footage, number of bedrooms, number of bathrooms, latitude, and longitiude. This is a regression problem since the outcome of interest (rent price) is continuous. To accomplish this task, we will fit the following families of models to the data: Linear regression, regression trees, and Random Forests. \n",
    "\n",
    "We randomly split the data into a learning set (80% of observations) to train the predictors and into a test set (20% of observations) to evaluate and compare their performances.\n",
    "\n",
    "The Craigslist data are prepared for analysis in the cells below. The results in this notebook will differ from the results shown in lecture due to the randomness induced by splitting the data into learning and test sets. Try changing the random seed to see how this will affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "craigslist_df = pd.read_csv(\"craigslist.csv\")\n",
    "\n",
    "# select response and required features\n",
    "craigslist_df = craigslist_df.loc[:, [\"price\", \"sqft\", \"bedrooms\", \"bath\", \"lat\", \"long\"]]\n",
    "\n",
    "# drop all observations with missing feature values\n",
    "craigslist_df.dropna(inplace=True)\n",
    "\n",
    "# remove the outliers in long and lat variables.\n",
    "# outliers are defined as values below Q1-1.5*IQR or above Q3+1.5*IQR\n",
    "# where Qi is the i^th quartile and IQR is the interquartile range\n",
    "Q1_lat = craigslist_df[\"lat\"].quantile(0.25)\n",
    "Q3_lat = craigslist_df[\"lat\"].quantile(0.75)\n",
    "IQR_lat = Q3_lat - Q1_lat\n",
    "Q1_long = craigslist_df[\"long\"].quantile(0.25)\n",
    "Q3_long = craigslist_df[\"long\"].quantile(0.75)\n",
    "IQR_long = Q3_long - Q1_long\n",
    "craigslist_df = craigslist_df.loc[(craigslist_df[\"lat\"] > Q1_lat - 1.5*IQR_lat) & \n",
    "                                  (craigslist_df[\"lat\"] < Q3_lat + 1.5*IQR_lat) &\n",
    "                                  (craigslist_df[\"long\"] > Q1_long - 1.5*IQR_long) & \n",
    "                                  (craigslist_df[\"long\"] < Q3_long + 1.5*IQR_long)]\n",
    "\n",
    "# remove observations with square footage below 100 or above 500\n",
    "craigslist_df = craigslist_df.loc[(craigslist_df[\"sqft\"] >= 100) & (craigslist_df[\"sqft\"] <= 5000)]\n",
    "\n",
    "craigslist_df.shape\n",
    "# partition data into test and learning set\n",
    "np.random.seed(210)\n",
    "X = craigslist_df.loc[:, [\"sqft\", \"bedrooms\", \"bath\", \"lat\", \"long\"]]\n",
    "y = craigslist_df.loc[:, \"price\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "In this section, we fit a linear regression function over all five covariates using the ordinary least squares method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the linear model from sklearn\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# initialize model\n",
    "linear_model = lm.LinearRegression(fit_intercept=True)\n",
    "\n",
    "# fit the model\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# compute the fitted and predicted values of the rental price\n",
    "y_fitted = linear_model.predict(X_train)\n",
    "y_predicted = linear_model.predict(X_test)\n",
    "\n",
    "# compute the learning and test set MSEs\n",
    "mse_train = np.mean((y_fitted - y_train)**2)\n",
    "mse_test = np.mean((y_predicted - y_test)**2)\n",
    "\n",
    "# compute the learning and test sets' coefficients of determination \n",
    "R2_train = r2_score(y_train, y_fitted)\n",
    "R2_test = r2_score(y_test, y_predicted)\n",
    "\n",
    "# display the results in a table\n",
    "table = [[\"Dataset\", \"MSE\", \"$R^2$\"],\n",
    "         [\"Learning\", round(mse_train, 3), round(R2_train, 3)],\n",
    "         [\"Test\", round(mse_test, 3), round(R2_test, 3)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationships between the fitted rent values and the features are visualized in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe and plot the results\n",
    "results_df = X_train.copy()\n",
    "results_df[\"fitted\"] = y_fitted\n",
    "g = sns.PairGrid(results_df, y_vars=\"fitted\", x_vars=[\"sqft\", \"bedrooms\", \"bath\", \"long\", \"lat\"], height=4)\n",
    "g.map(sns.regplot, fit_reg=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try using a subset of the features to see how this affects the learning and test set MSEs and $R^2$s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different set of features\n",
    "new_cov = [\"sqft\", \"lat\", \"long\"]\n",
    "X_train_new = X_train.loc[:, new_cov]\n",
    "X_test_new = X_test.loc[:, new_cov]\n",
    "\n",
    "# fit the model on a different set of covariates\n",
    "linear_model.fit(X_train_new, y_train)\n",
    "\n",
    "# compute the fitted and predicted values of the rental price\n",
    "y_fitted = linear_model.predict(X_train_new)\n",
    "y_predicted = linear_model.predict(X_test_new)\n",
    "\n",
    "# compute the learning and test set MSEs\n",
    "mse_train = np.mean((y_fitted - y_train)**2)\n",
    "mse_test = np.mean((y_predicted - y_test)**2)\n",
    "\n",
    "# compute the learning and test sets' coefficients of determination \n",
    "R2_train = r2_score(y_train, y_fitted)\n",
    "R2_test = r2_score(y_test, y_predicted)\n",
    "\n",
    "# display the results in a table\n",
    "table = [[\"Dataset\", \"MSE\", \"$R^2$\"],\n",
    "         [\"Learning\", round(mse_train, 3), round(R2_train, 3)],\n",
    "         [\"Test\", round(mse_test, 3), round(R2_test, 3)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Trees\n",
    "\n",
    "In this section, a regression tree is fit to the learning data using the CART method. The maximum depth of the tree is set to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the CART method\n",
    "from sklearn import tree\n",
    "\n",
    "# initialize mode\n",
    "clf = tree.DecisionTreeRegressor(max_depth=5)\n",
    "\n",
    "# fit the model\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# compute the fitted and predicted values of the rental price\n",
    "y_fitted = clf.predict(X_train)\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "# compute the learning and test set MSEs\n",
    "mse_train = np.mean((y_fitted - y_train)**2)\n",
    "mse_test = np.mean((y_predicted - y_test)**2)\n",
    "\n",
    "# compute the learning and test sets' coefficients of determination \n",
    "R2_train = r2_score(y_train, y_fitted)\n",
    "R2_test = r2_score(y_test, y_predicted)\n",
    "\n",
    "# display the results in a table\n",
    "table = [[\"Dataset\", \"MSE\", \"$R^2$\"],\n",
    "         [\"Learning\", round(mse_train, 3), round(R2_train, 3)],\n",
    "         [\"Test\", round(mse_test, 3), round(R2_test, 3)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we determine the feature importances. There are many ways to assess each features importance, but `sklearn` uses Gini importance (also called \"mean decrease in impurity\") in classification and regression trees, as well as in Random Forests. This is a normalized measurement; the importances sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [X_train.columns.values,\n",
    "         np.round(clf.feature_importances_, 3)]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indices = np.argsort(table[1])\n",
    "plt.title(\"Regression Tree Feature Importance\")\n",
    "plt.barh(y = range(len(indices)), width = table[1][indices],align=\"center\")\n",
    "plt.yticks(range(len(indices)), [table[0][i] for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the the regression tree\n",
    "!conda install -y graphviz\n",
    "import graphviz \n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try changing the number of features and the maximum depth of the tree in the previous cells. How does this affect the results? What other tuning parameters could you tweak to improve the performance of the tree? Check out the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor) for ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "This section uses Random Forests to predict rental costs of apartments in Berkeley. The maximum depth of each tree is again set to 5 and there are 100 trees in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random forest model from sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# set the hyperparameters\n",
    "rfor = RandomForestRegressor(max_depth=5, random_state=0, n_estimators=100)\n",
    "\n",
    "# fit the random forest to the learning data\n",
    "rfor = rfor.fit(X_train, y_train)\n",
    "\n",
    "# compute the fitted and predicted values of the rental price\n",
    "y_fitted = rfor.predict(X_train)\n",
    "y_predicted = rfor.predict(X_test)\n",
    "\n",
    "# compute the learning and test set MSEs\n",
    "mse_train = np.mean((y_fitted - y_train)**2)\n",
    "mse_test = np.mean((y_predicted - y_test)**2)\n",
    "\n",
    "# compute the learning and test sets' coefficients of determination \n",
    "R2_train = r2_score(y_train, y_fitted)\n",
    "R2_test = r2_score(y_test, y_predicted)\n",
    "\n",
    "# display the results in a table\n",
    "table = [[\"Dataset\", \"MSE\", \"$R^2$\"],\n",
    "         [\"Learning\", round(mse_train, 3), round(R2_train, 3)],\n",
    "         [\"Test\", round(mse_test, 3), round(R2_test, 3)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable importances are shown in the following table and figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [X_train.columns.values,\n",
    "         np.round(rfor.feature_importances_, 3)]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(table[1])\n",
    "plt.title(\"Random Forests Feature Importance\")\n",
    "plt.barh(y = range(len(indices)), width = table[1][indices],align=\"center\")\n",
    "plt.yticks(range(len(indices)), [table[0][i] for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests clearly outperforms a single regression tree and linear regression in terms of test set MSE and $R^2$. However, the importance of the features in the Random Forests and the regression tree are very similar.\n",
    "\n",
    "Try playing with the tuning parameters to see how this affects the Random Forests' performance. The documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying MNIST Handwritten Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import sklearn.datasets\n",
    "mnist = sklearn.datasets.load_digits()\n",
    "\n",
    "# format mnist data bunch into a dataframe and an array\n",
    "y = pd.Series(mnist.target).astype('int').astype('category')\n",
    "X = pd.DataFrame(mnist.data)\n",
    "\n",
    "# split the data into learning and testing sets\n",
    "np.random.seed(14)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.143, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "First, a multinomial logistic regression model is fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize the multinomial logistic regression model\n",
    "lr = lm.LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=4000)\n",
    "\n",
    "# fit the model to the digits data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels on the learning and testing data\n",
    "class_fitted = lr.predict(X_train)\n",
    "class_pred = lr.predict(X_test)\n",
    "\n",
    "# compute the accuracies\n",
    "train_acc = sum(class_fitted == y_train)/len(y_train)\n",
    "test_acc = sum(class_pred == y_test)/len(y_test)\n",
    "\n",
    "# print the results\n",
    "table = [[\"Dataset\", \"Accuracy\"],\n",
    "         [\"Learning\", round(train_acc, 3)],\n",
    "         [\"Test\", round(test_acc, 3)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multinomial logistic regression function classifies the handwritten digits with exceptional accuracy in the test set. The confusion matrix of the test set is presented in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_test = sklearn.metrics.confusion_matrix(y_test, class_pred)\n",
    "df_cm = pd.DataFrame(conf_mat_test, index = [i for i in \"0123456789\"],\n",
    "                  columns = [i for i in \"0123456789\"])\n",
    "cmap = sns.cubehelix_palette(light=0.95, as_cmap=True)\n",
    "sns.heatmap(df_cm, annot=True, cmap = cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbours (kNN)\n",
    "\n",
    "The second method used to clasify the MNIST handwritten digits data is k-nearest neighbours (kNN). The parameter k specifying the number of neighbours is set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model from sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# initialize the model\n",
    "neigh = KNeighborsClassifier(n_neighbors=100)\n",
    "\n",
    "# fit the model to the learning data\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels on the learning and testing data\n",
    "class_fitted = neigh.predict(X_train)\n",
    "class_pred = neigh.predict(X_test)\n",
    "\n",
    "# compute the accuracies\n",
    "train_acc = sum(class_fitted == y_train)/len(y_train)\n",
    "test_acc = sum(class_pred == y_test)/len(y_test)\n",
    "\n",
    "# print the results\n",
    "table = [[\"Dataset\", \"Accuracy\"],\n",
    "         [\"Learning\", round(train_acc, 3)],\n",
    "         [\"Test\", round(test_acc, 3)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kNN classification algorithm does a great job of predicting the handwritten digits! Sometimes the simplest models perform the best. What happens when you change the `n_neighbors` parameter to 10? To 1000? How can you explain the change in results? What would be a good approach for selecting the appropriate number of neighbours? \n",
    "\n",
    "The confusion matrix of the test set predictions are plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_test = sklearn.metrics.confusion_matrix(y_test, class_pred)\n",
    "df_cm = pd.DataFrame(conf_mat_test, index = [i for i in \"0123456789\"],\n",
    "                  columns = [i for i in \"0123456789\"])\n",
    "sns.heatmap(df_cm, annot=True, cmap = cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Trees\n",
    "\n",
    "Next, the handwritten digits are classified using classification trees. The maximum depth of the tree is set to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the classification tree model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# initialize the model\n",
    "clf = DecisionTreeClassifier(random_state=0, max_depth=5)\n",
    "\n",
    "# fit the model to the learning data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict the labels on the learning and testing data\n",
    "class_fitted = clf.predict(X_train)\n",
    "class_pred = clf.predict(X_test)\n",
    "\n",
    "# compute the accuracies\n",
    "train_acc = sum(class_fitted == y_train)/len(y_train)\n",
    "test_acc = sum(class_pred == y_test)/len(y_test)\n",
    "\n",
    "# print the results\n",
    "table = [[\"Dataset\", \"Accuracy\"],\n",
    "         [\"Learning\", round(train_acc, 3)],\n",
    "         [\"Test\", round(test_acc, 3)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "len(class_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classificiation tree is presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dot_data = tree.export_graphviz(clf, out_file=None)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid of pixel importances is presented below. As expected, the pixels near the center of each image (where the digits are drawn) are the most important in the classification tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_mat = np.reshape(clf.feature_importances_, (-1, 8))\n",
    "sns.heatmap(feat_imp_mat, linewidth=0.5, cmap = cmap)\n",
    "plt.title(\"Pixel Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions produced by the classification tree model aren't as accurate as those of kNN. Try changing the maximum depth of tree to see if you can improve the model's performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Random Forests are used to predict the classes of the MNIST data. The maximum depth is set to 5 and the number of trees is set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random forest model from sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set the hyperparameters\n",
    "rfor = RandomForestClassifier(max_depth=5, random_state=0, n_estimators=10)\n",
    "\n",
    "# fit the random forest to the learning data\n",
    "rfor = rfor.fit(X_train, y_train)\n",
    "\n",
    "# compute the fitted and predicted values of the rental price\n",
    "class_fitted = rfor.predict(X_train)\n",
    "class_pred = rfor.predict(X_test)\n",
    "\n",
    "# compute the accuracies\n",
    "train_acc = sum(class_fitted == y_train)/len(y_train)\n",
    "test_acc = sum(class_pred == y_test)/len(y_test)\n",
    "\n",
    "# print the results\n",
    "table = [[\"Dataset\", \"Accuracy\"],\n",
    "         [\"Learning\", round(train_acc, 3)],\n",
    "         [\"Test\", round(test_acc, 3)]]\n",
    "display(HTML(tabulate.tabulate(table, tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of each pixel in the model is presented below. Again, the pixels near the center play the most important roles in the Random Forests model. However, the feature importances are spread across a larger set of pixels than in the classification tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_mat = np.reshape(rfor.feature_importances_, (-1, 8))\n",
    "sns.heatmap(feat_imp_mat, linewidth=0.5, cmap = cmap)\n",
    "plt.title(\"Pixel Importance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rrandom Forests perform quite well. Try modifying the number of trees and/or the maximum depth to see if you can make it more accurate than kNN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
