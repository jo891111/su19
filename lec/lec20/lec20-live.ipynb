{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import warnings\n",
    "# Ignore numpy dtype warnings. These warnings are caused by an interaction\n",
    "# between numpy and Cython and can be safely ignored.\n",
    "# Reference: https://stackoverflow.com/a/40846742\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('talk')\n",
    "np.set_printoptions(threshold=20, precision=2, suppress=True)\n",
    "pd.set_option('display.max_rows', 7)\n",
    "pd.set_option('display.max_columns', 9)\n",
    "pd.set_option('precision', 2)\n",
    "# This option stops scientific notation for pandas\n",
    "# pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "def df_interact(df, nrows=7, ncols=7):\n",
    "    '''\n",
    "    Outputs sliders that show rows and columns of df\n",
    "    '''\n",
    "    def peek(row=0, col=0):\n",
    "        return df.iloc[row:row + nrows, col:col + ncols]\n",
    "\n",
    "    row_arg = (0, len(df), nrows) if len(df) > nrows else fixed(0)\n",
    "    col_arg = ((0, len(df.columns), ncols)\n",
    "               if len(df.columns) > ncols else fixed(0))\n",
    "    \n",
    "    interact(peek, row=row_arg, col=col_arg)\n",
    "    print('({} rows, {} columns) total'.format(df.shape[0], df.shape[1]))\n",
    "\n",
    "def display_df(df, rows=50, cols=pd.options.display.max_columns):\n",
    "    with pd.option_context('display.max_rows', rows,\n",
    "                           'display.max_columns', cols):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('games.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(confusion):\n",
    "    sns.heatmap(confusion, annot=True, fmt='d',\n",
    "                cmap=\"Blues\", annot_kws={'fontsize': 24}, square=True,\n",
    "                xticklabels=[1, 0], yticklabels=[1, 0], cbar=False)\n",
    "    plt.xlabel('True')\n",
    "    plt.ylabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You try: Draw the ROC curve for this small dataset of 5 points. Then compute the AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "X_small = X.sample(5)\n",
    "y_small = y[X_small.index]\n",
    "y_hat_small = clf.predict_proba(X_small)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(thres=(0, 1, 0.1))\n",
    "def adjust_thres(thres=1):\n",
    "    pred = (y_hat_small >= thres).astype(int)\n",
    "    df = (pd.DataFrame({'y': y_small, 'y_hat': y_hat_small, 'pred': pred})\n",
    "          .sort_values('y_hat'))\n",
    "    fpr = df.loc[df['y'] == 0, 'pred'].mean()\n",
    "    tpr = df.loc[df['y'] == 1, 'pred'].mean()\n",
    "    print(f'fpr: {fpr:.2f}       tpr: {tpr:.2f}')\n",
    "    display_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_small, y_hat_small)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.scatter(fpr, tpr)\n",
    "plt.xticks([0, 1/3, 2/3, 1], ['0', r'$\\frac{1}{3}$', r'$\\frac{2}{3}$', r'1'])\n",
    "plt.yticks([0, 0.5, 1], ['0', r'$\\frac{1}{2}$', r'1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression summary\n",
    "\n",
    "The logistic or sigmoid function can be written two equivalent ways:\n",
    "\n",
    "$$\\sigma(t) = \\frac{1}{1 + \\exp(-t)} = \\frac{\\exp(t)}{1 + \\exp(t)} $$\n",
    "\n",
    "The logistic regression model assumes the following probabilities of $Y \\in \\{0, 1\\}$ given column vector $X$:\n",
    "\n",
    "\\begin{align*}\n",
    "P(Y=1|X) &= \\sigma(X^T \\theta) &&= \\frac{1}{1 + \\exp(-X^T \\theta)} &= \\frac{\\exp(X^T\\theta)}{1 + \\exp(X^T\\theta)} \\\\[10pt]\n",
    "P(Y=0|X) &= \\sigma(-X^T \\theta) &&= \\frac{1}{1 + \\exp(X^T \\theta)}  \\\\\n",
    "\\end{align*}\n",
    "\n",
    "The loss most typically used to fit $\\theta$ is the log loss or cross-entropy loss, which is the negative log probability of the correct (observed) $Y$ value. This loss for true $Y \\in \\{0, 1\\}$ and predicted probability $\\hat Y \\in [0, 1]$ is often written:\n",
    "\n",
    "$$-Y \\log(\\hat Y) - (1-Y)\\log(1- \\hat Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"The logistic or sigmoid function, denoted Ïƒ(t).\n",
    "    \n",
    "    Note: This is actually a special case of what is generally \n",
    "          named the \"logistic\" function,\n",
    "          which allows for a different numerator and offset, \n",
    "          but lots of people call this the logistic function in practice.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def prediction(x, theta):\n",
    "    \"\"\"Prediction under the logistic model for features x and parameters b.\"\"\"\n",
    "    pass\n",
    "\n",
    "def squared_loss(y, y_hat):\n",
    "    \"\"\"Squared loss applies to any true y and predicted y_hat.\"\"\"\n",
    "    pass\n",
    "\n",
    "def log_loss(y, y_hat):\n",
    "    \"\"\"Log loss or cross-entropy loss, assuming y is in [0, 1].\"\"\"\n",
    "    pass\n",
    "\n",
    "def empirical_risk(true_ys, predicted_ys, loss):\n",
    "    \"\"\"The empirical risk is the average loss for a sample.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Risk\n",
    "\n",
    "Filling in $\\hat Y = P(Y=1|X)$ and filling in the form of the model, we find different ways of expressing the same loss:\n",
    "\n",
    "\\begin{align*}\n",
    "L(\\theta) &= -Y \\log(\\hat Y) - (1-Y)\\log(1- \\hat Y) \\\\[10pt]\n",
    "         &= - Y \\log P(Y=1|X) - (1-Y) \\log P(Y=0|X)  \\\\[10pt]\n",
    "         &= - Y \\log \\frac{\\exp(X \\cdot \\theta)}{1 + \\exp(X \\cdot \\theta)} - (1-Y) \\log \\frac{1}{1 + \\exp(X \\cdot \\theta)}  \\\\[10pt]\n",
    "         &= - Y (\\log(\\exp(X \\cdot \\theta)) - \\log(1 + \\exp(X \\cdot \\theta))) - (1-Y) (-\\log (1 + \\exp(X \\cdot \\theta)))  \\\\[10pt]\n",
    "         &= - YX \\cdot \\theta + Y \\log(1 + \\exp(X \\cdot \\theta))) - Y \\log(1 + \\exp(X \\cdot \\theta))) + \\log (1 + \\exp(X \\cdot \\theta))  \\\\[10pt]\n",
    "         &= - YX \\cdot \\theta + \\log (1 + \\exp(X \\cdot \\theta)) \\\\[10pt]\n",
    "         &= -\\left(YX \\cdot \\theta + \\log \\sigma(-X \\cdot \\theta)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Where the last step follows from $\\log (1 + \\exp(X \\cdot \\theta)) = -(- \\log (1 + \\exp(X \\cdot \\theta))) = -\\log \\frac{1}{1 + \\exp(X \\cdot \\theta)} = -\\log \\sigma(-X \\cdot \\theta)$.\n",
    "\n",
    "The empirical risk (average loss across a sample) for a set of observations $(x_1, y_1) \\dots (x_n, y_n)$ is often written:\n",
    "\n",
    "$$R(\\theta, x, y) = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i x_i \\cdot \\theta + \\log \\sigma(-x_i \\cdot \\theta) \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Gradient\n",
    "\n",
    "Using the logistic regression model and log loss, find the gradient of the empirical risk.\n",
    "\n",
    "First, we compute the derivative of the sigmoid function since we'll use it in our gradient calculation.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(t) &= \\frac{1}{1 + e^{-t}} \\\\[10pt]\n",
    "\\sigma'(t) &= \\frac{e^{-t}}{(1 + e^{-t})^2} \\\\[10pt]\n",
    "\\sigma'(t) &= \\frac{1}{1 + e^{-t}} \\cdot \\left(1 - \\frac{1}{1 + e^{-t}} \\right) \\\\[10pt]\n",
    "\\sigma'(t) &= \\sigma(t) (1 - \\sigma(t))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a shorthand, we define $ \\sigma_i = \\sigma(-x_i\\cdot \\theta) $. We will soon need the gradient of $ \\sigma_i $ with respect to the vector $ \\theta $ so we will derive it now using the chain rule. \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\theta} \\sigma_i\n",
    "&= \\nabla_{\\theta} \\sigma(-x_i\\cdot \\theta) \\\\[10pt]\n",
    "&= \\sigma\\left(-x_i\\cdot \\theta\\right) \\left(1 - \\sigma(-x_i\\cdot \\theta)\\right)  \\nabla_{\\theta} \\left(-x_i\\cdot \\theta\\right) \\\\[10pt]\n",
    "&= -\\sigma_i (1 - \\sigma_i) x_i \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we derive the gradient of the cross-entropy loss with respect to the model parameters $ \\boldsymbol{\\theta} $. We use the fact that $(1-\\sigma_i) = \\sigma(x_i \\cdot \\theta)$, since $\\sigma(x \\cdot \\theta) + \\sigma(-x \\cdot \\theta) = 1$.\n",
    "\n",
    "\\begin{align*}\n",
    "R(\\theta, x, y) &= - \\frac{1}{n}\\sum_{i=1}^n \\left[ y_i x_i \\cdot \\theta + \\log \\sigma_i \\right] \\\\[10pt]\n",
    "\\nabla_{\\theta} R(\\theta, x, y) &= - \\frac{1}{n}\\sum_{i=1}^n \\left( y_i x_i - \\frac{1}{\\sigma_i} \\sigma_i (1 - \\sigma_i) x_i \\right) \\\\[10pt]\n",
    "                              &= - \\frac{1}{n}\\sum_{i=1}^n \\left( y_i x_i - \\sigma(x_i \\cdot \\theta) x_i \\right) \\\\[10pt]\n",
    "                              &= - \\frac{1}{n}\\sum_{i=1}^n \\left(y_i - \\sigma(x_i \\cdot \\theta)\\right) x_i  \\\\[10pt]\n",
    "\\end{align*}\n",
    "\n",
    "The gradient for a single point is the expression inside the summation. This lets us run SGD if we so desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, theta0, gradient_function, max_iter=1000000,  \n",
    "                     epsilon=1e-8, lr=5, clip=1):\n",
    "    \"\"\"Run gradient descent on a dataset (x, y) \n",
    "    with gradient clipping and learning rate decay.\"\"\"\n",
    "    theta = theta0\n",
    "    pass\n",
    "    return theta\n",
    "\n",
    "def risk_gradient(theta, x, y):\n",
    "    \"\"\"Risk gradient for a whole dataset at once.\"\"\"\n",
    "    pass\n",
    "\n",
    "def logistic_regression(x, y):\n",
    "    \"\"\"Train a logistic regression classifier using gradient descent.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "data_dict = sklearn.datasets.load_breast_cancer()\n",
    "cancer = pd.DataFrame(data_dict['data'], columns=data_dict['feature_names'])\n",
    "cancer['bias'] = 1.0\n",
    "# Target data_dict['target'] = 0 is malignant; 1 is benign\n",
    "cancer['malignant'] = 1 - data_dict['target']\n",
    "cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(cancer, test_size=0.25, random_state=100)\n",
    "print(\"Training Data Size: \", len(train))\n",
    "print(\"Test Data Size: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radii = np.linspace(5, 30, 50)\n",
    "averages = [np.average(train[np.abs(train['mean radius']-r)<2]['malignant']) for r in radii]\n",
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(t):\n",
    "    pass\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(fit_intercept=False, C=1e9, solver='saga')\n",
    "model.fit(x_train, y_train)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');\n",
    "plt.plot(radii, sigmoid(model.coef_[0,0] + radii * model.coef_[0,1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train['mean radius'], train['malignant']);\n",
    "plt.scatter(radii, averages, color='gold');\n",
    "plt.plot(radii, sigmoid(theta[0] + radii * theta[1]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking test set performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_features(t):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(theta, features):\n",
    "    print(f'Train auc: {auc(features(train), y_train, theta):.3f}')\n",
    "    print(f' Test auc: {auc(features(test), y_test, theta):.3f}')\n",
    "    \n",
    "evaluate(theta, all_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
